{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Large-scale data analysis with Spacy (by. huffon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이번 챕터에서는 큰 용량의 데이터에서 원하는 정보를 추출하는 방법을 배워볼 것입니다.\n",
    "- 해당 튜토리얼을 통해 `spaCy` 라이브러리의 자료구조들을 생성하는 법과, \n",
    "- 어떻게 하면 효과적으로 통계 기반과 규칙 기반 분석 방법을 결합할 수 있을지에 대하여 학습하게 될 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Structures (1): Vocab, Lexemes and StringStore\n",
    "#### Shared vocab and string store (1)\n",
    "- `Vocab`: doc 객체들 간 공유하는 단어를 저장하는 자료구조\n",
    "- 메모리를 절약하기 위해, spaCy는 모든 문자열을 해시 값으로 인코드\n",
    "- 문자열은 `nip.vocab.strings`를 통해 `StringStore`에 단 한 번 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value: ', nlp.vocab.strings['coffee'])\n",
    "print('string value: ', nlp.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print('hash value: ', doc.vocab.strings['coffee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `lexeme`는 vocab의 최소 단위 객체로 문맥에 독립적인 단어 정보들을 포함\n",
    "    - 단어 정보의 예: `lexeme.text`와 `lexeme.orth` (해시 값) \n",
    "    - `lexeme.is_alpha`와 같은 어휘 속성 역시 포함\n",
    "\n",
    "<br/>\n",
    "<img src='https://course.spacy.io/vocab_stringstore.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "print('{} / {} / {}'.format(lexeme.text, lexeme.orth, lexeme.is_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Strings to hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 해시 값을 얻기 위해 문자열 **PERSON**을 `nlp.vocab.strings`에 대입해봅시다.\n",
    "- 문자열을 얻기 위해 **PERSON**의 해시 값을 다시 대입해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"David Bowie is a PERSON\")\n",
    "\n",
    "person_hash = nlp.vocab.strings[\"PERSON\"]\n",
    "print(person_hash)\n",
    "\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Structures (2): Doc, Span and Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Doc 객체 임포트\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# 단어 리스트와 공백 리스트 정의\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# 앞서 정의한 리스트들을 이용해 doc 객체 수동으로 생성\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Span object\n",
    "\n",
    "<img src='https://course.spacy.io/span_indices.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc과 Span 클래스 임포트\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# 단어 리스트와 공백 리스트 정의\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# 위에서 했던 것과 같이 Doc 객체 수동으로 정의\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Doc 객체 이용해 Span 객체를 수동으로 정의\n",
    "span = Span(doc, 0, 2)\n",
    "print(span)\n",
    "\n",
    "# Span 객체에 라벨 부여\n",
    "span_with_label = Span(doc, 0, 2, label=\"Greeting\")\n",
    "\n",
    "# doc 객체의 엔티티로 생성한 Span 추가\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Doc`과 `Span` 은 매우 강력한 객체입니다.\n",
    "    - 해당 객체들로 오래오래 사용하고, 최대한 나중에 string으로 변환합시다 !\n",
    "- Token 객체의 속성을 최대한 활용하세요 ! (e.g. token.i를 사용해 index 추출 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doc 객체에서 사용된 단어들의 저장을 위해 vocab을 Doc에 넘겨주는 것을 잊지마세요 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 목표 텍스트: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# 단어, 공백 리스트 + vocab 이용해 Doc 객체 수동 생성\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"David Bowie\"를 나타내는 `span` 객체를 생성 후, `\"PERSON\"` 라벨을 부여하세요\n",
    "- \"David Bowie\" 를 나타내는 `span` 객체를 doc.ents에 덮어쓰세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# 단어, 공백 리스트 + vocab 이용해 Doc 객체 수동 생성\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# \"David Bowie\"를 커버하는 Span 객체를 doc으로부터 생성 후, \"PERSON\" 라벨 부여\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, '/', span.label_)\n",
    "\n",
    "# Span 객체를 doc 엔티티에 추가\n",
    "doc.ents = [span]\n",
    "\n",
    "# 엔티티의 텍스트와 라벨 속성을 출력\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word vectors and semantic similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `spaCy`는 두 객체를 비교해 **유사도** 예측 가능\n",
    "- `Doc.similarity()`, `Span.similarity()`, `Token.similarity()`\n",
    "- 위 메소드들은 인자로 다른 객체를 받으며, `0-1` 사이의 유사도를 반환\n",
    "- **주의**: word vector를 포함하고 있는 크기의 모델을 다운로드 받아야 함\n",
    "    - `en_core_web_md` 혹은 `en_core_web_lg`\n",
    "    - Small model은 word vector 포함하지 않음 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vector를 포함하는 크기의 모델 로드\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# 두 개의 객체를 비교해 유사도 추출\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pizza와 pasta 토큰 비교\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token, Doc, Span 등 서로 다른 객체 간 비교도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc과 Token의 비교\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Span과 Doc의 비교\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This was a great restaurant. Afterwards, we went to a really nice bar.\")\n",
    "\n",
    "# \"great restaurant\" Span과 \"really nice bar\" Span 정의\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[12:15]\n",
    "print(span1, '/', span2)\n",
    "\n",
    "# Span 간 유사도 계산\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy는 어떻게 유사도를 측정하고 있을까요?\n",
    "- 유사도는 **Word vectors**를 이용해 결정됩니다.\n",
    "- 이는 단어들의 의미를 다차원 공간에 나타낸 수치입니다.\n",
    "- Word vector는 **Word2Vec**을 비롯한 다양한 알고리즘을 이용해 생성할 수 있습니다.\n",
    "- 또한 외부 알고리즘을 통해 구한 유사도를 spaCy의 통계 모델에 추가할 수도 있습니다.\n",
    "- spaCy에서 유사도를 구하는 기본 척도는 **코사인 유사도**이지만, 이는 변경이 가능합니다.\n",
    "- `Doc`과 `Span` 객체의 유사도는 토큰 유사도의 평균으로 정의됩니다.\n",
    "- 긴 문장보다 작은 문장의 유사도가 더 정확히 구해지는데, 이는 문장이 길어질수록 유사도를 구하는데 방해가 되는 단어들이 많아지기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have a banana\")\n",
    "# token.vector 속성 통해 word vector 출력 가능\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유사도의 정의는 사용되는 어플리케이션에 따라 다를 수 있음\n",
    "- 문맥과 어플리케이션의 목적에 따라 유사도의 정의가 달라져야 함 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    "# Is it similar or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Combining models and rules\n",
    "- 통계 모델은 일반화 된 분석 결과를 반환해주지만, 모든 경우를 포함할 수는 없기에 규칙 기반의 시스템과 결합되어 사용되는 것이 바람직함\n",
    "- `spaCy`에서는 `Matcher`, `PhraseMatcher` 클래스 등을 이용해 외부 규칙들을 더해줄 수 있음\n",
    "- Matcher 클래스 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy import displacy\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "displacy.serve(doc, style=\"dep\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 효과적인 구(Phrase) 매칭 방법\n",
    "- `PhraseMatcher`를 사용하면 보다 효율적으로 **구**를 매칭할 수 있음\n",
    "- 이때 `Doc` 객체를 패턴으로 취함\n",
    "- `Matcher` 클래스보다 빠르고 효과적 !\n",
    "    - 때로는 개별 토큰을 추출하는 패턴을 작성하는 것보다 문자열 그대로를 매칭하는  `PhraseMatcher`을 사용하는 것이 더 효과적일 때가 있음\n",
    "        - e.g.) 국가명과 같은 유한한 범주에서 문자열을 추출하고자 할 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\") # Doc -> pattern\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# 매칭 결과를 순회\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # 매칭 결과를 Span 객체로 생성\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
