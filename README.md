# Deep Learning with NLP 101
**ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ë¶€í„° ìµœì‹  íë¦„ì„ ì´í•´í•˜ê¸° ê¹Œì§€, ë…¼ë¬¸ê³¼ íŠœí† ë¦¬ì–¼ì„ í†µí•´ ë‹¬ë ¤ë³´ëŠ” ìŠ¤í„°ë”” !** 

### ì„ ìˆ˜ì§€ì‹
- íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ê¸°ë³¸ ë¬¸ë²•
- ë”¥ëŸ¬ë‹ ê¸°ë³¸ ì§€ì‹: **'ëª¨ë‘ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹'** ê°•ì˜ë¥¼ ì™„ê°•í•˜ì‹  ê²½í—˜ë§Œ ìˆìœ¼ë©´ ì¶©ë¶„í•©ë‹ˆë‹¤ !
- ìŠ¤í„°ë”” ê¸°ê°„ ì¤‘ **ìµœì†Œ í•œ ë²ˆ** ì´ìƒ ë°œì œë¥¼ í•  ìš©ê¸°

<br/>

### í•¨ê»˜í•˜ëŠ” ì‚¬ëŒ
[**í—ˆ í›ˆ**](https://github.com/Huffon), [**ì´ì¸í™˜**](https://github.com/lih0905), ë‚¨í˜œë¦¬, ë°•ë³‘ì¤€, [**ì •ë¯¼ìˆ˜**](https://github.com/4seaday), ê¹€ì§„ì›, ì •ì§€ìš©, [**í—ˆë¬´ì§€**](https://github.com/Moo-Ji), [**ì´ëª…í•™**](https://github.com/myeonghak), ì „ë‹¤í•´, [**ì •ë¯¼ì†Œ**](https://github.com/minssoj), [**ì„ì†¡í˜„**](https://github.com/shyun46)

<br/>

### ìŠ¤í„°ë”” ë‚œì´ë„
ğŸŒğŸŒğŸŒ—ğŸŒšğŸŒš

<br/>

### ë°°ìš°ê²Œ ë  ê²ƒ
1. **Theory**: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ì˜ ì „ë°˜ì  íë¦„ì„ ë…¼ë¬¸ê³¼ ì•„í‹°í´ì„ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.
2. **Hands-on**: ì•„ì§ í•œê¸€ì„ ì§€ì›í•˜ì§€ëŠ” ì•Šì§€ë§Œ, ìµœê·¼ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” `spaCy` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ íŠœí† ë¦¬ì–¼ì„ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.
3. **Hands-on**: `PyTorch` Tutorialì´ ì œê³µí•˜ëŠ” ìì—°ì–´ ì²˜ë¦¬ ì‹¤ìŠµ ì½”ë“œë¥¼ ëŒë ¤ë³´ë©°, ì‹¤ì œ ìì—°ì–´ ì²˜ë¦¬ í”„ë¡œê·¸ë¨ì´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í•™ìŠµë˜ëŠ”ì§€ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤.

<br/>

## Month 1
ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ì˜ ë°œì „ ê³¼ì •ê³¼ ì´ì— ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ì ì¸ ëª¨ë¸ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.

---

- Week 1: [Recent Trends in Deep Learning Based Natural Language Processing](https://arxiv.org/pdf/1708.02709.pdf)
- Week 2: [Recurrent Neural Network](https://ko.coursera.org/lecture/nlp-sequence-models/recurrent-neural-network-model-ftkzt) + spaCy Chapter 1: Finding words, phrases, names and concepts ( ëª…í•™ / ì§€ìš© )
- Week 3: [Long Short-Term Memory](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) + spaCy Chapter 2: Large-scale data analysis with spaCy ( ì¸í™˜ / í›ˆ )
- Week 4: [Gated Recurrent Unit](https://arxiv.org/pdf/1412.3555.pdf) + spaCy Chapter 3: Processing Pipelines ( ë¬´ì§€ / í›ˆ )

<br/>

## Month 2
ì»´í“¨í„°ì—ê²Œ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´ì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”í•œ ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•ì„ ë°œì „ìˆœìœ¼ë¡œ ì•Œì•„ë´…ë‹ˆë‹¤.

---

- Week 5: [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) + spaCy Chapter 4: Training a neural network model ( ë³‘ì¤€ / ë¯¼ì†Œ )
- Week 6: [Glove](https://nlp.stanford.edu/pubs/glove.pdf) + DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ ( ë¯¼ìˆ˜ / ëª…í•™ )
- Week 7: [Fasttext](https://arxiv.org/pdf/1607.01759.pdf) + [ELMo](https://arxiv.org/pdf/1802.05365.pdf) ( í˜œë¦¬ / ë¬´ì§€ )
- Week 8: [Word-Piece Model](https://arxiv.org/pdf/1609.08144.pdf) + DEEP LEARNING FOR NLP WITH PYTORCH #1 ( ì§€ìš© / ì§„ì› )

<br/>

## Month 3
ìˆ˜ ë§ì€ ìì—°ì–´ ì²˜ë¦¬ Task ì¤‘ ê¸°ê³„ë²ˆì—­ Taskì˜ ë°œì „ì‚¬ë¥¼ ë…¼ë¬¸ì„ í†µí•´ ì•Œì•„ë´…ë‹ˆë‹¤.

---

- Week 9: [Sequence-to-Sequence](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) [(PyTorch Implementation)](https://github.com/Huffon/pytorch-seq2seq-kor-eng) + DEEP LEARNING FOR NLP WITH PYTORCH #2 ( í›ˆ / ì¸í™˜ )
- Week 10: [Attention](https://arxiv.org/pdf/1409.0473.pdf) + What is torch.nn really? ( ì†¡í˜„ / í›ˆ )
- Week 11: [Transformer](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) + Annotated Transformer ( ì¸í™˜ /  )
- Week 12: [Natural Language Processing with. CNN](https://arxiv.org/pdf/1408.5882.pdf) + Closing partyğŸ‰ ( ë‹¤í•´ / all )
